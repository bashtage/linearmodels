# Python package
# Create and test a Python package on multiple Python versions.
# Add steps that analyze code, save the dist with the build record, publish to a PyPI-compatible index, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

parameters:
  # defaults for any parameters that are not specified
  name: ''
  vmImage: ''


jobs:

- job: ${{ parameters.name }}Test
  pool:
    vmImage: ${{ parameters.vmImage }}
  strategy:
    matrix:
      python310_minimums:
        python.version: '3.10'
        NUMPY: 1.22.3
        SCIPY: 1.8.0
        PANDAS: 1.3.0
        STATSMODELS: 0.13.1
        XARRAY: 0.21.0
        FORMULAIC: 1.0.2
        test.install: true
      python310_mid:
        python.version: '3.10'
        NUMPY: 1.23.0
        SCIPY: 1.9.0
        PANDAS: 1.4.0
        STATSMODELS: 0.13.1
        XARRAY: 2022.6.0
        XXHASH: true
        FORMULAIC: 1.0.2
        test.install: true
      python310_recent:
        python.version: '3.10'
        NUMPY: 1.24.0
        SCIPY: 1.12.0
        PANDAS: 2.0.0
        STATSMODELS: 0.14.0
        XARRAY: 2023.4.0
        FORMULAIC: 1.1.0
        test.install: true
      python310_latest:
        python.version: '3.10'
        FORMULAIC: 1.2.0
        XXHASH: true
        PYARROW: true
      python310_no_cython:
        python.version: '3.10'
        LM_NO_BINARY: 1
      python311_latest:
        python.version: '3.11'
        XXHASH: true
        PYARROW: true
      python312_latest:
        python.version: '3.12'
        XXHASH: true
        PYARROW: true
      python313_latest:
        python.version: '3.13'
        XXHASH: true
        PYARROW: true
      python313_copy_on_write:
        python.version: '3.12'
        XXHASH: true
        LM_TEST_COPY_ON_WRITE: 1
      python312_pre:
        python.version: '3.12'
        pip.pre: true
    maxParallel: 10

  steps:
  - task: UsePythonVersion@0
    inputs:
      versionSpec: '$(python.version)'
      architecture: 'x64'
    displayName: 'Use Python $(python.version)'

  - script: |
      python -m pip install --upgrade pip setuptools>=61 wheel
      python -m pip install -r requirements.txt
      python -m pip install -r requirements-test.txt
      python -m pip install -r requirements-dev.txt
      source ci/install-posix.sh      
      jupyter kernelspec list
    displayName: 'Install dependencies'

  - script: |
      python -m pip list
    displayName: 'List Configuration'

  - script: |
     flake8 linearmodels
     black --check linearmodels
     isort --check linearmodels
     ruff check linearmodels
    displayName: 'Check style and formatting'

  - script: |
      echo "Installing to site packages"
      python -m pip wheel . --wheel-dir ./dist/ --no-build-isolation
      WHL=$(ls -t ./dist/linearmodels-*.whl | head -1)
      pip install ${WHL}
    displayName: 'Install linearmodels (site-packages)'
    condition: eq(variables['test.install'], 'true')

  - script: |
      echo python -m pip install -e . -v --no-build-isolation
      python -m pip install -e . -v --no-build-isolation
    displayName: 'Install linearmodels (editable)'
    condition: ne(variables['test.install'], 'true')

  - script: |
      echo "Testing site packages"
      mkdir test_run_dir
      pushd test_run_dir
      python -c "import linearmodels; linearmodels.test(['-n', 'auto', '--junitxml=../junit/test-results.xml'])"
      popd
    displayName: 'Run tests (site-packages)'
    condition: and(eq(variables['test.install'], 'true'), ne(variables['pip.pre'], 'true'))

  - script: |
      echo "Testing editable install"
      if [[ ${COVERAGE} == "true" ]]; then
        export COVERAGE_OPTS="--cov-config .coveragerc --cov=linearmodels --cov-report xml:coverage.xml --cov-report term"
      fi
      echo pytest -m "${PYTEST_PATTERN}" --junitxml=junit/test-results.xml -n auto --durations=25 ${COVERAGE_OPTS} linearmodels/tests
      pytest -m "${PYTEST_PATTERN}" --junitxml=junit/test-results.xml -n auto --durations=25 ${COVERAGE_OPTS} linearmodels/tests
    displayName: 'Run tests (editable)'
    condition: and(ne(variables['test.install'], 'true'), ne(variables['pip.pre'], 'true'))

  - script: |
      echo "Testing pip-pre"
      if [[ ${COVERAGE} == "true" ]]; then
        export COVERAGE_OPTS="--cov-config .coveragerc --cov=linearmodels --cov-report xml:coverage.xml --cov-report term"
      fi
      echo pytest -m "${PYTEST_PATTERN}" --junitxml=junit/test-results.xml -n auto --durations=25 ${COVERAGE_OPTS} linearmodels/tests
      pytest -m "${PYTEST_PATTERN}" --junitxml=junit/test-results.xml -n auto --durations=25 ${COVERAGE_OPTS} linearmodels/tests
    displayName: 'Run tests (pip pre)'
    condition: eq(variables['pip.pre'], 'true')
    continueOnError: true

  - task: PublishTestResults@2
    inputs:
      testResultsFiles: '**/test-results.xml'
      testRunTitle: 'Python $(python.version)'
    condition: succeededOrFailed()

  - task: PublishCodeCoverageResults@2
    inputs:
      summaryFileLocation: '$(System.DefaultWorkingDirectory)/**/coverage.xml'
    condition: and(eq(variables['coverage'], 'true'), ne(variables['test.install'], 'true'))

  - bash: bash <(curl -s https://codecov.io/bash)
    displayName: 'CodeCov upload'
    condition: and(eq(variables['coverage'], 'true'), ne(variables['test.install'], 'true'))
