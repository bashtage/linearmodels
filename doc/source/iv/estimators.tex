%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}
\usepackage{babel}
\begin{document}

\section*{Mathematical Formulas}

\subsection*{Notation}

Interest is in recovering the parameter vector from the model 
\begin{align*}
y_{i} & =\beta^{\prime}x_{i}+\epsilon_{i}
\end{align*}

\noindent where $\beta$ is $k$ by 1, $x_{i}$ is a $k$ by 1 vector
of observable variables and $\epsilon_{i}$ is a scalar error. $x_{i}$
can be separated in two types of variables. The $k_{1}$ by 1 set
of variables $x_{1i}$ are exogenous regressors in the sense that
$E\left[x_{1i}\epsilon_{i}\right]=0$. The $k_{2}$ by 1 variables
$x_{2i}$ are endogenous. A set of $p$ instruments is available that
satisfy the requirements for validity where $p\geq k_{2}$. The extended
model can be written as 
\begin{align*}
y_{i} & =\underset{\textrm{exogenous}}{\underbrace{\beta_{1}^{\prime}x_{1i}}}+\underset{\textrm{endogenous}}{\underbrace{\beta_{2}^{\prime}x_{2i}}}+\epsilon_{i}\\
x_{2i} & =\underset{\textrm{exogenous}}{\underbrace{\gamma_{1}^{\prime}z_{1i}}}+\underset{\textrm{instruments}}{\underbrace{\gamma_{2}^{\prime}z_{2i}}}+u_{i}
\end{align*}

The model can be expressed compactly 
\begin{align*}
Y & =X_{1}\beta_{1}+X_{2}\beta_{1}+\epsilon=X\beta+\epsilon\\
X_{2} & =Z_{1}\gamma_{1}+Z_{2}\gamma_{2}+u=Z\gamma+u
\end{align*}

\noindent The vector of instruments $z_{i}$ is $p$ by 1. There are
$n$ observations for all variables. $k_{c}=1$ if the model contains
a constant (either explicit or implicit, i.e., including all categories
of a dummy variable). The constant, if included, is in $x_{1i}$.
$X$ is the $n$ by $k$ matrix if regressors where row $i$ of $X$
is $x_{i}^{\prime}$. $X$ can be partitioned into $\left[X_{1}\;X_{2}\right]$.
$Z$ is the $n$ by $p$ matrix of instruments. The vector $y$ is
$n$ by 1. Projection matrices for $X$ is defined $P_{X}=X\left(X^{\prime}X\right)^{-1}X^{\prime}$.
The projection matrix for $Z$ is similarly defined only using $Z$.
The annihilator matrix for $X$ is $M_{X}=I-P_{X}$.

\subsection*{Parameter Estimation}

\subsubsection*{Two-stage Least Squares (2SLS)}

The 2SLS estimator is 
\begin{align*}
\hat{\beta}_{2SLS} & =\left(X^{\prime}P_{Z}X^{\prime}\right)\left(X^{\prime}P_{Z}y^{\prime}\right)
\end{align*}


\subsubsection*{Limited Information Maximum Likelihood and k-class Estimators}

The LIML or other k-class estimator is 
\begin{align*}
\hat{\beta}_{\kappa} & =\left(X^{\prime}\left(I-\kappa M_{Z}\right)X^{\prime}\right)\left(X^{\prime}\left(I-\kappa M_{Z}\right)y^{\prime}\right)
\end{align*}

\noindent where $\kappa$ is the parameter of the class. When $\kappa=1$
the 2SLS estimator is recovered. When $\kappa=0$, the OLS estimator
is recovered. The LIML estimator is recovered for $\kappa$ set to
\[
\min\mathrm{eig\left[\left(W^{\prime}M_{z}W\right)^{-\frac{1}{2}}\left(W^{\prime}M_{X_{1}}W\right)\left(W^{\prime}M_{z}W\right)^{-\frac{1}{2}}\right]}
\]

\noindent where $W=\left[y\:X_{2}\right]$ and $\mathrm{eig}$ returns
the eigenvalues. \textbf{TODO: Check this}

\subsubsection*{Generalized Method of Moments (GMM)}

The GMM estimator is defined as 
\begin{align*}
\hat{\beta}_{GMM} & =\left(X^{\prime}ZWZ^{\prime}X\right)^{-1}\left(X^{\prime}ZWZ^{\prime}y\right)
\end{align*}

\noindent where $W$ is a positive definite weighting matrix. 

\subsection*{Basic Statistics}

Let $\hat{\epsilon}=y-X\hat{\beta}$. The residual sum of squares
(RSS) is $\hat{\epsilon}^{\prime}\hat{\epsilon}$, the model sum of
squares (MSS) is $\hat{\beta}^{\prime}X^{\prime}X\hat{\beta}$ and
the total sum of squares (TSS) is $\left(y-k_{c}\bar{y}\right)^{\prime}\left(y-k_{c}\bar{y}\right)^{\prime}$where
$\bar{y}$ is the sample average of $y$. The model $R^{2}$is defined
\begin{align*}
R^{2} & =1-\frac{\hat{\epsilon}^{\prime}\hat{\epsilon}}{\left(y-k_{c}\bar{y}\right)^{\prime}\left(y-k_{c}\bar{y}\right)^{\prime}}=1-\frac{RSS}{TSS}
\end{align*}

\noindent and the adjusted $R^{2}$ is defined 
\begin{align*}
\bar{R}^{2} & =1-\left(1-R^{2}\right)\frac{N-k_{c}}{N-k}.
\end{align*}

\noindent The residual variance is $s^{2}=n\hat{\epsilon}^{\prime}\hat{\epsilon}$
unless the debiased flag is used, in which case a small sample adjusted
version is estimated $s^{2}=\left(n-k\right)^{-1}\hat{\epsilon}^{\prime}\hat{\epsilon}$.

The model F-statistic is defined 
\begin{align*}
F & =\dot{\hat{\beta}}^{\prime}\hat{V}^{-1}\dot{\hat{\beta}}
\end{align*}

\noindent where the notation $\dot{\hat{\beta}}$ indicates that the
constant is excluded from $\hat{\beta}$ and $\dot{\hat{V}}$ indicates
that the row and column corresponding to a constant are excluded.\footnote{If the model contains an implicit constant, e.g., all categories of
a dummy, one of the categories is excluded when computing the test
statistic. The choice of category to drop has no effect and is equivalent
to reparameterizing the model with a constant and excluding one category
of dummy.} The test statistic is distributed as $\chi_{k-k_{c}}^{2}.$ If the
debiased flag is set, then the test statistic is transformed as $F/\left(k-k_{c}\right)$
and a $F_{k-k_{c},n-k}$ distribution is used. 

\subsection*{Parameter Covariance Estimation}

\subsubsection*{Two-stage LS, LIML and k-class estimators}

Four covariance estimators are available. The first is the standard
homoskedastic covariance, defined as 

\begin{align*}
n^{-1}s^{2}\left(\frac{X^{\prime}\left(I-\kappa M_{z}\right)X}{n}\right)^{-1} & =n^{-1}s^{2}\hat{A}.
\end{align*}

Note that this estimator can be expressed as 
\begin{align*}
n^{-1}\hat{\hat{A}^{-1}}\left\{ s^{2}\hat{A}\right\} \hat{A}^{-1} & =n^{-1}\hat{A}^{-1}\hat{B}\hat{A}^{-1}.
\end{align*}

\noindent All estimator take this form and only differ in how the
asymptotic covariance of the scores, $B$, is estimated. For the homoskedastic
covariance estimator, $\hat{B}=s^{2}\hat{A}.$ The score covariance
in the heteroskedasticity robust covariance estimator is 
\begin{align*}
\hat{B} & =n^{-1}\sum_{i=1}^{n}\hat{\epsilon}_{i}^{2}\hat{x}_{i}\hat{x}_{i}^{\prime}=n^{-1}\sum_{i=1}^{n}\hat{\xi}_{i}\hat{\xi}_{i}^{\prime}.
\end{align*}

\noindent where $\hat{x_{i}}$ is row $i$ of $\hat{X}=P_{Z}X$ and
$\hat{\xi}_{i}=\hat{\epsilon}_{i}\hat{x}_{i}$.

The kernel covariance estimator is robust to both heteroskedasticity
and autocorrelation and is defined as 
\begin{align*}
\hat{B} & =\hat{\Gamma}_{0}+\sum_{i=1}^{n-1}k\left(i/h\right)\left(\hat{\Gamma}_{i}+\hat{\Gamma}_{i}^{\prime}\right)\\
\hat{\Gamma_{j}} & =n^{-1}\sum_{i=j+1}^{n}\hat{\xi}_{i-j}\hat{\xi}_{i}^{\prime}
\end{align*}

\noindent where $k\left(i/h\right)$ is a kernel weighting function
where $h$ is the kernel bandwidth. 

The one-way clustered covariance estimator is defined as 
\begin{align*}
n^{-1}\sum_{j=1}^{g}\left(\sum_{i\in\mathcal{G}_{j}}\hat{\xi}_{i}\right)\left(\sum_{i\in\mathcal{G}_{j}}\hat{\xi}_{i}\right)^{\prime}
\end{align*}

\noindent where $\sum_{i\in\mathcal{G}_{j}}\hat{\xi}_{i}$ is the
sum of the scores for all members in group $\mathcal{G}_{j}$ and
$g$ is the number of groups. 

If the debiased flag is used to perform a small-sample adjustment,
all estimators except the clustered covariance are rescaled by $\left(n-k\right)/n$.
The clustered covariance is rescaled by $\left(\left(n-k\right)\left(n-1\right)/n^{2}\right)\left(\left(g-1\right)/g\right)$.\footnote{This somewhat non-obvious choice is drive by Stata compatibility.}
\end{document}
