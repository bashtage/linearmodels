

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  
  <!-- Licensed under the Apache 2.0 License -->
  <link rel="stylesheet" type="text/css" href="../_static/fonts/open-sans/stylesheet.css" />
  <!-- Licensed under the SIL Open Font License -->
  <link rel="stylesheet" type="text/css" href="../_static/fonts/source-serif-pro/source-serif-pro.css" />
  <link rel="stylesheet" type="text/css" href="../_static/css/bootstrap.min.css" />
  <link rel="stylesheet" type="text/css" href="../_static/css/bootstrap-theme.min.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
    <title>Formulas and Mathematical Detail &#8212; linearmodels 4.8+11.g0239039 documentation</title>
    <link rel="stylesheet" href="../_static/guzzle.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Panel Data Model Estimation" href="../panel/index.html" />
    <link rel="prev" title="GMM Weight and Covariance Estimation" href="gmm.html" />
  
  
   

  <link rel="stylesheet" href="../_static/css/overrides.css" type="text/css" />

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../panel/index.html" title="Panel Data Model Estimation"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="gmm.html" title="GMM Weight and Covariance Estimation"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">linearmodels 4.8+11.g0239039 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U">Instrumental Variable Estimation</a> &#187;</li> 
      </ul>
    </div>
    <div class="container-wrapper">

      <div id="mobile-toggle">
        <a href="#"><span class="glyphicon glyphicon-align-justify" aria-hidden="true"></span></a>
      </div>
  <div id="left-column">
    <div class="sphinxsidebar"><a href="
    ../index.html" class="text-logo">linearmodels 4.8 (+11, g0239039)</a>
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <h2>Table Of Contents</h2>
  </div>
  <div class="sidebar-toc">
    
    
      <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Instrumental Variable Estimation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/basic-examples.html">Basic Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/advanced-examples.html">Further Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/using-formulas.html">Using formulas to specify models</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference.html">Module Reference</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Formulas and Mathematical Detail</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../panel/index.html">Panel Data Model Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asset-pricing/index.html">Linear Factor Models for Asset Pricing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../system/index.html">System Regression Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utility.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plan.html">Module Plans</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes.html">Change Log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
</ul>

    
  </div>
</div>
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <div id="main-search">
      <form class="form-inline" action="../search.html" method="GET" role="form">
        <div class="input-group">
          <input name="q" type="text" class="form-control" placeholder="Search...">
        </div>
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div>
      
    </div>
  </div>
        <div id="right-column">
          
          <div role="navigation" aria-label="breadcrumbs navigation">
            <ol class="breadcrumb">
              <li><a href="../index.html">Docs</a></li>
              
                <li><a href="index.html">Instrumental Variable Estimation</a></li>
              
              <li>Formulas and Mathematical Detail</li>
            </ol>
          </div>
          
          <div class="document clearer body">
            
  <div class="section" id="formulas-and-mathematical-detail">
<span id="iv-mathematical-notation"></span><h1>Formulas and Mathematical Detail<a class="headerlink" href="#formulas-and-mathematical-detail" title="Permalink to this headline">¶</a></h1>
<div class="section" id="notation">
<h2>Notation<a class="headerlink" href="#notation" title="Permalink to this headline">¶</a></h2>
<p>Interest is in recovering the parameter vector from the model</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
y_{i} &amp; =\beta^{\prime}x_{i}+\epsilon_{i}\end{aligned}\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is <span class="math notranslate nohighlight">\(k\)</span> by 1, <span class="math notranslate nohighlight">\(x_{i}\)</span> is a <span class="math notranslate nohighlight">\(k\)</span> by 1
vector of observable variables and <span class="math notranslate nohighlight">\(\epsilon_{i}\)</span> is a scalar
error. <span class="math notranslate nohighlight">\(x_{i}\)</span> can be separated in two types of variables. The
<span class="math notranslate nohighlight">\(k_{1}\)</span> by 1 set of variables <span class="math notranslate nohighlight">\(x_{1i}\)</span> are exogenous
regressors in the sense that <span class="math notranslate nohighlight">\(E\left[x_{1i}\epsilon_{i}\right]=0\)</span>.
The <span class="math notranslate nohighlight">\(k_{2}\)</span> by 1 variables <span class="math notranslate nohighlight">\(x_{2i}\)</span> are endogenous. A set of
<span class="math notranslate nohighlight">\(p\)</span> instruments is available that satisfy the requirements for
validity where <span class="math notranslate nohighlight">\(p\geq k_{2}\)</span>. The extended model can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
y_{i} &amp; =\underset{\textrm{exogenous}}{\underbrace{\beta_{1}^{\prime}x_{1i}}}+\underset{\textrm{endogenous}}{\underbrace{\beta_{2}^{\prime}x_{2i}}}+\epsilon_{i}\\
x_{2i} &amp; =\underset{\textrm{exogenous}}{\underbrace{\gamma_{1}^{\prime}z_{1i}}}+\underset{\textrm{instruments}}{\underbrace{\gamma_{2}^{\prime}z_{2i}}}+u_{i}\end{aligned}\end{split}\]</div>
<p>The model can be expressed compactly</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
Y &amp; =X_{1}\beta_{1}+X_{2}\beta_{1}+\epsilon=X\beta+\epsilon\\
X_{2} &amp; =Z_{1}\gamma_{1}+Z_{2}\gamma_{2}+u=Z\gamma+u\end{aligned}\end{split}\]</div>
<p>The vector of instruments <span class="math notranslate nohighlight">\(z_{i}\)</span> is <span class="math notranslate nohighlight">\(p\)</span> by 1. There are
<span class="math notranslate nohighlight">\(n\)</span> observations for all variables. <span class="math notranslate nohighlight">\(k_{c}=1\)</span> if the model
contains a constant (either explicit or implicit, i.e., including all
categories of a dummy variable). The constant, if included, is in
<span class="math notranslate nohighlight">\(x_{1i}\)</span>. <span class="math notranslate nohighlight">\(X\)</span> is the <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(k\)</span> matrix if
regressors where row <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(x_{i}^{\prime}\)</span>.
<span class="math notranslate nohighlight">\(X\)</span> can be partitioned into <span class="math notranslate nohighlight">\(\left[X_{1}\;X_{2}\right]\)</span>.
<span class="math notranslate nohighlight">\(Z\)</span> is the <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(p\)</span> matrix of instruments. The
vector <span class="math notranslate nohighlight">\(y\)</span> is <span class="math notranslate nohighlight">\(n\)</span> by 1. Projection matrices for <span class="math notranslate nohighlight">\(X\)</span> is
defined <span class="math notranslate nohighlight">\(P_{X}=X\left(X^{\prime}X\right)^{-1}X^{\prime}\)</span>. The
projection matrix for <span class="math notranslate nohighlight">\(Z\)</span> is similarly defined only using
<span class="math notranslate nohighlight">\(Z\)</span>. The annihilator matrix for <span class="math notranslate nohighlight">\(X\)</span> is
<span class="math notranslate nohighlight">\(M_{X}=I-P_{X}\)</span>.</p>
</div>
<div class="section" id="parameter-estimation">
<h2>Parameter Estimation<a class="headerlink" href="#parameter-estimation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="two-stage-least-squares-2sls">
<h3>Two-stage Least Squares (2SLS)<a class="headerlink" href="#two-stage-least-squares-2sls" title="Permalink to this headline">¶</a></h3>
<p>The 2SLS estimator is</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\hat{\beta}_{2SLS} &amp; =\left(X^{\prime}P_{Z}X\right)^{-1}\left(X^{\prime}P_{Z}y\right)\end{aligned}\]</div>
</div>
<div class="section" id="limited-information-maximum-likelihood-and-k-class-estimators">
<h3>Limited Information Maximum Likelihood and k-class Estimators<a class="headerlink" href="#limited-information-maximum-likelihood-and-k-class-estimators" title="Permalink to this headline">¶</a></h3>
<p>The LIML or other k-class estimator is</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\hat{\beta}_{\kappa} &amp; =\left(X^{\prime}\left(I-\kappa M_{Z}\right)X\right)^{-1}\left(X^{\prime}\left(I-\kappa M_{Z}\right)y\right)\end{aligned}\]</div>
<p>where <span class="math notranslate nohighlight">\(\kappa\)</span> is the parameter of the class. When
<span class="math notranslate nohighlight">\(\kappa=1\)</span> the 2SLS estimator is recovered. When <span class="math notranslate nohighlight">\(\kappa=0\)</span>,
the OLS estimator is recovered. The LIML estimator is recovered when
<span class="math notranslate nohighlight">\(\kappa\)</span> set to</p>
<div class="math notranslate nohighlight">
\[\hat{\kappa}=\min\mathrm{eig\left[\left(W^{\prime}M_{Z}W\right)^{-\frac{1}{2}}\left(W^{\prime}M_{X_{1}}W\right)\left(W^{\prime}M_{Z}W\right)^{-\frac{1}{2}}\right]}\]</div>
<p>where <span class="math notranslate nohighlight">\(W=\left[y\:X_{2}\right]\)</span> and <span class="math notranslate nohighlight">\(\mathrm{eig}\)</span> returns
the eigenvalues.</p>
</div>
<div class="section" id="generalized-method-of-moments-gmm">
<h3>Generalized Method of Moments (GMM)<a class="headerlink" href="#generalized-method-of-moments-gmm" title="Permalink to this headline">¶</a></h3>
<p>The GMM estimator is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\hat{\beta}_{GMM} &amp; =\left(X^{\prime}ZWZ^{\prime}X\right)^{-1}\left(X^{\prime}ZWZ^{\prime}y\right)\end{aligned}\]</div>
<p>where <span class="math notranslate nohighlight">\(W\)</span> is a positive definite weighting matrix.</p>
</div>
<div class="section" id="continuously-updating-generalized-method-of-moments-gmm-cue">
<h3>Continuously Updating Generalized Method of Moments (GMM-CUE)<a class="headerlink" href="#continuously-updating-generalized-method-of-moments-gmm-cue" title="Permalink to this headline">¶</a></h3>
<p>The continuously updating GMM estimator solves the minimization problem</p>
<div class="math notranslate nohighlight">
\[\min_{\beta}n\bar{g}\left(\beta\right)^{\prime}W\left(\beta\right)^{-1}\bar{g}\left(\beta\right)\]</div>
<p>where
<span class="math notranslate nohighlight">\(\bar{g}\left(\beta\right)=n^{-1}\sum_{i=1}^{n}z_{i}\hat{\epsilon}_{i}\)</span>
and <span class="math notranslate nohighlight">\(\hat{\epsilon}_{i}=y_{i}-x_{i}\beta\)</span>. Unlike standard GMM,
the weight matrix, <span class="math notranslate nohighlight">\(W\)</span> directly depends on the model parameters
<span class="math notranslate nohighlight">\(\beta\)</span> and so it is not possible to use a closed form estimator.</p>
</div>
</div>
<div class="section" id="basic-statistics">
<h2>Basic Statistics<a class="headerlink" href="#basic-statistics" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\hat{\epsilon}=y-X\hat{\beta}\)</span>. The residual sum of squares
(RSS) is <span class="math notranslate nohighlight">\(\hat{\epsilon}^{\prime}\hat{\epsilon}\)</span>, the model sum of
squares (MSS) is <span class="math notranslate nohighlight">\(\hat{\beta}^{\prime}X^{\prime}X\hat{\beta}\)</span> and
the total sum of squares (TSS) is
<span class="math notranslate nohighlight">\(\left(y-k_{c}\bar{y}\right)^{\prime}\left(y-k_{c}\bar{y}\right)^{\prime}\)</span>where
<span class="math notranslate nohighlight">\(\bar{y}\)</span> is the sample average of <span class="math notranslate nohighlight">\(y\)</span>. The model
<span class="math notranslate nohighlight">\(R^{2}\)</span>is defined</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
R^{2} &amp; =1-\frac{\hat{\epsilon}^{\prime}\hat{\epsilon}}{\left(y-k_{c}\bar{y}\right)^{\prime}\left(y-k_{c}\bar{y}\right)^{\prime}}=1-\frac{RSS}{TSS}\end{aligned}\]</div>
<p>and the adjusted <span class="math notranslate nohighlight">\(R^{2}\)</span> is defined</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\bar{R}^{2} &amp; =1-\left(1-R^{2}\right)\frac{N-k_{c}}{N-k}.\end{aligned}\]</div>
<p>The residual variance is
<span class="math notranslate nohighlight">\(s^{2}=n^{-1}\hat{\epsilon}^{\prime}\hat{\epsilon}\)</span> unless the
debiased flag is used, in which case a small sample adjusted version is
estimated
<span class="math notranslate nohighlight">\(s^{2}=\left(n-k\right)^{-1}\hat{\epsilon}^{\prime}\hat{\epsilon}\)</span>.
The model degree of freedom is <span class="math notranslate nohighlight">\(k\)</span> and the residual degree of
freedom is <span class="math notranslate nohighlight">\(n-k\)</span>.</p>
<p>The model F-statistic is defined</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
F &amp; =\hat{\beta}_{-}^{\prime}\hat{V}_{-}^{-1}\dot{\hat{\beta}_{-}}\end{aligned}\]</div>
<p>where the notation <span class="math notranslate nohighlight">\(\hat{\beta}_{-}\)</span> indicates that the constant
is excluded from <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> and <span class="math notranslate nohighlight">\(\hat{V}_{-}\)</span> indicates
that the row and column corresponding to a constant are excluded. <a class="footnote-reference" href="#id3" id="id1">[1]</a>
The test statistic is distributed as <span class="math notranslate nohighlight">\(\chi_{k-k_{c}}^{2}.\)</span> If the
debiased flag is set, then the test statistic <span class="math notranslate nohighlight">\(F\)</span> is transformed
as <span class="math notranslate nohighlight">\(F/\left(k-k_{c}\right)\)</span> and a <span class="math notranslate nohighlight">\(F_{k-k_{c},n-k}\)</span>
distribution is used.</p>
</div>
<div class="section" id="parameter-covariance-estimation">
<h2>Parameter Covariance Estimation<a class="headerlink" href="#parameter-covariance-estimation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="two-stage-ls-liml-and-k-class-estimators">
<h3>Two-stage LS, LIML and k-class estimators<a class="headerlink" href="#two-stage-ls-liml-and-k-class-estimators" title="Permalink to this headline">¶</a></h3>
<p>Four covariance estimators are available. The first is the standard
homoskedastic covariance, defined as</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\hat{\Sigma}=n^{-1}s^{2}\left(\frac{X^{\prime}\left(I-\kappa M_{z}\right)X}{n}\right)^{-1} &amp; =n^{-1}s^{2}\hat{A}.\end{aligned}\]</div>
<p>Note that this estimator can be expressed as</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\hat{\Sigma}=n^{-1}\hat{A}^{-1}\left\{ s^{2}\hat{A}\right\} \hat{A}^{-1} &amp; =n^{-1}\hat{A}^{-1}\hat{B}\hat{A}^{-1}.\end{aligned}\]</div>
<p>All estimators take this form and only differ in how the asymptotic
covariance of the scores, <span class="math notranslate nohighlight">\(B\)</span>, is estimated. For the homoskedastic
covariance estimator, <span class="math notranslate nohighlight">\(\hat{B}=s^{2}\hat{A}.\)</span> The score covariance
in the heteroskedasticity robust covariance estimator is</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\hat{B} &amp; =n^{-1}\sum_{i=1}^{n}\hat{\epsilon}_{i}^{2}\hat{x}_{i}\hat{x}_{i}^{\prime}=n^{-1}\sum_{i=1}^{n}\hat{\xi}_{i}\hat{\xi}_{i}^{\prime}.\end{aligned}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{x_{i}}\)</span> is row <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(\hat{X}=P_{Z}X\)</span> and
<span class="math notranslate nohighlight">\(\hat{\xi}_{i}=\hat{\epsilon}_{i}\hat{x}_{i}\)</span>.</p>
<p>The kernel covariance estimator is robust to both heteroskedasticity and
autocorrelation and is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\hat{B} &amp; =\hat{\Gamma}_{0}+\sum_{i=1}^{n-1}K\left(\frac{i}{h}\right)\left(\hat{\Gamma}_{i}+\hat{\Gamma}_{i}^{\prime}\right)\\
\hat{\Gamma_{j}} &amp; =n^{-1}\sum_{i=j+1}^{n}\hat{\xi}_{i-j}\hat{\xi}_{i}^{\prime}\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(K\left(\frac{i}{h}\right)\)</span> is a kernel weighting function
where <span class="math notranslate nohighlight">\(h\)</span> is the kernel bandwidth.</p>
<p>The one-way clustered covariance estimator is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
n^{-1}\sum_{j=1}^{g}\left(\sum_{i\in\mathcal{G}_{j}}\hat{\xi}_{i}\right)\left(\sum_{i\in\mathcal{G}_{j}}\hat{\xi}_{i}\right)^{\prime}\end{aligned}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sum_{i\in\mathcal{G}_{j}}\hat{\xi}_{i}\)</span> is the sum of the
scores for all members in group <span class="math notranslate nohighlight">\(\mathcal{G}_{j}\)</span> and <span class="math notranslate nohighlight">\(g\)</span> is
the number of groups.</p>
<p>If the debiased flag is used to perform a small-sample adjustment, all
estimators except the clustered covariance are rescaled by
<span class="math notranslate nohighlight">\(\left(n-k\right)/n\)</span>. The clustered covariance is rescaled by
<span class="math notranslate nohighlight">\(\left(\left(n-k\right)\left(n-1\right)/n^{2}\right)\left(\left(g-1\right)/g\right)\)</span>. <a class="footnote-reference" href="#id4" id="id2">[2]</a></p>
</div>
<div class="section" id="standard-errors">
<h3>Standard Errors<a class="headerlink" href="#standard-errors" title="Permalink to this headline">¶</a></h3>
<p>Standard errors are defined as</p>
<div class="math notranslate nohighlight">
\[s.e.\left(\hat{\beta}_{j}\right)=\sqrt{e_{j}^{\prime}\hat{\Sigma}e_{j}}\]</div>
<p>where <span class="math notranslate nohighlight">\(e_{j}\)</span> is a vector of 0s except in location <span class="math notranslate nohighlight">\(j\)</span> which
is 1.</p>
</div>
<div class="section" id="t-statistics">
<h3>T-statistics<a class="headerlink" href="#t-statistics" title="Permalink to this headline">¶</a></h3>
<p>T-statistics test the null <span class="math notranslate nohighlight">\(H_{0}:\beta_{j}=0\)</span> against a 2-sided
alternative and are defined as</p>
<div class="math notranslate nohighlight">
\[z=\frac{\hat{\beta}_{j}}{s.e.\left(\hat{\beta}_{j}\right)}.\]</div>
</div>
<div class="section" id="p-values">
<h3>P-values<a class="headerlink" href="#p-values" title="Permalink to this headline">¶</a></h3>
<p>P-values are computes using 2-sided tests,</p>
<div class="math notranslate nohighlight">
\[Pr\left(\left|z\right|&gt;Z\right)=2-2\Phi\left(\left|z\right|\right)\]</div>
<p>If the covariance estimator was debiased, a Student’s t distribution
with <span class="math notranslate nohighlight">\(n-k\)</span> degrees of freedom is used,</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
Pr\left(\left|z\right|&gt;Z\right) &amp; =2-2t_{n-k}\left(\left|z\right|\right)\end{aligned}\]</div>
<p>where <span class="math notranslate nohighlight">\(t_{n-k}\left(\cdot\right)\)</span> is the CDF of a Student’s T
distribution.</p>
</div>
<div class="section" id="confidence-intervals">
<h3>Confidence Intervals<a class="headerlink" href="#confidence-intervals" title="Permalink to this headline">¶</a></h3>
<p>Confidence intervals are constructed as</p>
<div class="math notranslate nohighlight">
\[CI_{i,1-\alpha}=\hat{\beta}_{i}\pm q_{\alpha/2}\times\hat{\sigma}_{\beta_{i}}\]</div>
<p>where <span class="math notranslate nohighlight">\(q_{\alpha/2}\)</span> is the <span class="math notranslate nohighlight">\(\alpha/2\)</span> quantile of a
standard Normal distribution or a Student’s t. The Student’s t is used
when a debiased covariance estimator is used.</p>
</div>
<div class="section" id="linear-hypothesis-tests">
<h3>Linear Hypothesis Tests<a class="headerlink" href="#linear-hypothesis-tests" title="Permalink to this headline">¶</a></h3>
<p>Linear hypothesis tests examine the validity of nulls of the form
<span class="math notranslate nohighlight">\(H_{0}:R\beta-r=0\)</span> and are implemented using a Wald test statistic</p>
<div class="math notranslate nohighlight">
\[W=\left(R\hat{\beta}-r\right)^{\prime}\left[R^{\prime}\hat{\Sigma}R\right]^{-1}\left(R\hat{\beta}-r\right)\sim\chi_{q}^{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(q\)</span> is the <span class="math notranslate nohighlight">\(rank\left(R\right)\)</span> which is usually the
number of rows in <span class="math notranslate nohighlight">\(R\)</span> . If the debiased flag is used, then
<span class="math notranslate nohighlight">\(W/q\)</span> is reported and critical and p-values are taken from a
<span class="math notranslate nohighlight">\(F_{q,n-k}\)</span> distribution.</p>
</div>
<div class="section" id="gmm-covariance-estimators">
<h3>GMM Covariance estimators<a class="headerlink" href="#gmm-covariance-estimators" title="Permalink to this headline">¶</a></h3>
<p>GMM covariance depends on the weighting matrix used in estimation and
the assumed covariance of the scores. In most applications these are the
same and so the inefficient form,</p>
<div class="math notranslate nohighlight">
\[\hat{\Sigma}=n^{-1}\left(\left(\frac{X'Z}{n}\right)W\left(\frac{Z'X}{n}\right)\right)^{-1}\left(\left(\frac{X'Z}{n}\right)WSW\left(\frac{Z'X}{n}\right)\right)\left(\left(\frac{X'Z}{n}\right)W\left(\frac{Z'X}{n}\right)\right)^{-1}\]</div>
<p>will collapse to the simpler form</p>
<div class="math notranslate nohighlight">
\[\hat{\Sigma}=n^{-1}\left(\left(\frac{X'Z}{n}\right)W\left(\frac{Z'X}{n}\right)\right)^{-1}\]</div>
<p>when <span class="math notranslate nohighlight">\(W=S^{-1}\)</span>. When an unadjusted (homoskedastic) covariance is
used,</p>
<div class="math notranslate nohighlight">
\[\hat{S}=\tilde{s}^{2}n^{-1}\sum_{j=1}^{n}z_{j}^{\prime}z_{j}\]</div>
<p>where
<span class="math notranslate nohighlight">\(\tilde{s}^{2}=n^{-1}\sum_{i=1}^{n}\left(\epsilon_{i}-\bar{\epsilon}\right)^{2}\)</span>
subtracts the mean which may be non-zero if the model is overidentified.
Like previous covariance estimators, if the debiased flag is used,
<span class="math notranslate nohighlight">\(n^{-1}\)</span> is replaced by <span class="math notranslate nohighlight">\(\left(n-k\right)^{-1}\)</span>. When a
robust (heteroskedastic) covariance is used, the estimator of <span class="math notranslate nohighlight">\(S\)</span>
is modified to</p>
<div class="math notranslate nohighlight">
\[\hat{S}=n^{-1}\sum_{i=1}^{n}\hat{\epsilon}_{i}^{2}z_{i}^{\prime}z_{i}.\]</div>
<p>If the debiased flag is used, <span class="math notranslate nohighlight">\(n^{-1}\)</span> is replaced by
<span class="math notranslate nohighlight">\(\left(n-k\right)^{-1}\)</span>.</p>
<p>Kernel covariance estimators of <span class="math notranslate nohighlight">\(S\)</span> take the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\hat{S} &amp; =\hat{\Gamma}_{0}+\sum_{i=1}^{n-1}k\left(i/h\right)\left(\hat{\Gamma}_{i}+\hat{\Gamma}_{i}^{\prime}\right)\\
\hat{\Gamma_{j}} &amp; =n^{-1}\sum_{i=j+1}^{n}\hat{\epsilon}_{i-j}\hat{\epsilon}_{i}z_{i-j}^{\prime}z_{i}\end{aligned}\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(k\left(\cdot\right)\)</span> is a kernel weighting function with
bandwidth <span class="math notranslate nohighlight">\(h\)</span>. If the debiased flag is used, <span class="math notranslate nohighlight">\(n^{-1}\)</span> is
replaced by <span class="math notranslate nohighlight">\(\left(n-k\right)^{-1}\)</span>.</p>
<p>The one-way clustered covariance estimator is defined as</p>
<div class="math notranslate nohighlight">
\[\hat{S}=n^{-1}\sum_{j=1}^{g}\left(\sum_{i\in\mathcal{G}_{j}}\hat{\epsilon}_{i}z_{i}\right)^{\prime}\left(\sum_{i\in\mathcal{G}_{j}}\hat{\epsilon}_{i}z_{i}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\sum_{i\in\mathcal{G}_{j}}\hat{\epsilon}_{i}z_{i}\)</span> is the
sum of the moment conditional for all members in group
<span class="math notranslate nohighlight">\(\mathcal{G}_{j}\)</span> and <span class="math notranslate nohighlight">\(g\)</span> is the number of groups. If the
debiased flag is used, the <span class="math notranslate nohighlight">\(n^{-1}\)</span> term is replaced by</p>
<div class="math notranslate nohighlight">
\[\left(n-k\right)^{-1}\frac{n-1}{n}\frac{g}{g-1}.\]</div>
</div>
<div class="section" id="gmm-weight-estimators">
<h3>GMM Weight Estimators<a class="headerlink" href="#gmm-weight-estimators" title="Permalink to this headline">¶</a></h3>
<p>The GMM optimal weight estimators are identical to the the estimators of
<span class="math notranslate nohighlight">\(S\)</span> with two notable exceptions. First, they are never debiased
and so always use <span class="math notranslate nohighlight">\(n^{-1}\)</span>. Second, if the center flag is true,
the demeaned moment conditions defined as
<span class="math notranslate nohighlight">\(\tilde{g}_{i}=z_{i}\hat{\epsilon}_{i}-\overline{z\epsilon}\)</span> are
used in-place of <span class="math notranslate nohighlight">\(g_{i}\)</span> in the robust, kernel and clustered
estimators. The unadjusted estimator is always centered, and so this
option has no effect.</p>
</div>
</div>
<div class="section" id="post-estimation">
<h2>Post-estimation<a class="headerlink" href="#post-estimation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="sls-and-liml-post-estimation-results">
<h3>2SLS and LIML Post-estimation Results<a class="headerlink" href="#sls-and-liml-post-estimation-results" title="Permalink to this headline">¶</a></h3>
<p><strong>Sargan</strong></p>
<p>Sargan’s test of over-identifying restrictions examines whether the
variance of the IV residuals is similar to that of the OLS residuals.
The test statistic is computed</p>
<div class="math notranslate nohighlight">
\[s=n(1-\hat{\epsilon}^{\prime}M_{Z}\hat{\epsilon}/\hat{\epsilon}^{\prime}\hat{\epsilon})\sim\chi_{v}^{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\epsilon}\)</span> are the IV residuals and <span class="math notranslate nohighlight">\(M_{Z}\)</span> is
the annihilator matrix using all exogenous variables.<span class="math notranslate nohighlight">\(\nu\)</span> is
the number of overidentifying restrictions, which is the number of
instruments minus the number of endogenous variables, <span class="math notranslate nohighlight">\(p-k_{2}\)</span>.</p>
<p><strong>Basmann</strong></p>
<p>Basmann’s test is a small-sample corrected version of Sargan’s test of
over-identifying restrictions. It has the same distribution. Let
<span class="math notranslate nohighlight">\(s\)</span> be Sargan’s test statistic, then Basmann’s test statistic is</p>
<div class="math notranslate nohighlight">
\[s(n-p)/(n-s)\sim\chi_{v}^{2}\]</div>
<p><strong>Wooldridge score</strong></p>
<p>Wooldridge’s score test of exogeneity examines the magnitude of the
correlation between the OLS residuals and a an appropriately constructed
set of residuals of the instruments. Define <span class="math notranslate nohighlight">\(e=M_{X}Y\)</span> and
<span class="math notranslate nohighlight">\(\nu=M_{X}M_{Z}X_{2}\)</span>. Then the test statistic is computed from
the regression</p>
<div class="math notranslate nohighlight">
\[1=\gamma_{1}\hat{\epsilon}_{1}\hat{v}_{1,i}+\ldots+\gamma_{p}\hat{\epsilon}_{1}\hat{v}_{p,i}+\eta_{i}\]</div>
<p>as <span class="math notranslate nohighlight">\(nR^{2}\sim\chi_{k_{2}}^{2}\)</span>.</p>
<p><strong>Wooldridge regression</strong></p>
<p>Wooldridge’s regression test of exogeneity is closely related to the
score test and is generally more powerful. It also inherits robustness
to heteroskedasticity and/or autocorrelation the comes from the choice
of covariance estimator used in the model. Define <span class="math notranslate nohighlight">\(R=M_{Z}X_{2}\)</span>.
The test is implemented in a regression of</p>
<div class="math notranslate nohighlight">
\[Y=X\beta+R\gamma+\eta\]</div>
<p>as</p>
<div class="math notranslate nohighlight">
\[\hat{\gamma}^{\prime}\hat{\Sigma}_{\gamma}^{-1}\gamma^{\prime}\sim\chi_{k_{2}}^{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\Sigma}_{\gamma}\)</span> is the block of the covariance
matrix corresponding to the <span class="math notranslate nohighlight">\(\gamma\)</span> parameters.
<span class="math notranslate nohighlight">\(\hat{\Sigma}\)</span> is estimated using the same covariance estimator as
the model fit.</p>
<p><strong>Wooldridge’s Test of Overidentifying restrictions</strong></p>
<p>Wooldridge’s test is a score test examining whether the component of the
instrument that is uncorrelated with both the included exogenous and the
fitted exogenous is uncorrelated with the IV residuals. Define
<span class="math notranslate nohighlight">\(\tilde{Z}=M_{\left[X_{1}\:\hat{X}_{2}\right]}Z_{2,1:q}\)</span> where
<span class="math notranslate nohighlight">\(\hat{X_{2}}\)</span> are the fitted values from the first stage
regression of the endogenous on all exogenous variables and
<span class="math notranslate nohighlight">\(Z_{2,1:q}\)</span> contains any <span class="math notranslate nohighlight">\(q\)</span> columns of <span class="math notranslate nohighlight">\(Z_{2}\)</span>,
<span class="math notranslate nohighlight">\(q=p-k_{2}\)</span> . The test statistic is computed using a regression of
1s on the test functions <span class="math notranslate nohighlight">\(\hat{\epsilon}_{i}\tilde{z}_{i,j}\)</span> for
<span class="math notranslate nohighlight">\(j=1,\ldots,q\)</span> which should have expected value 0.</p>
<div class="math notranslate nohighlight">
\[1=\gamma_{1}\hat{\epsilon}_{i}\tilde{z}_{i,1}+\ldots+\gamma_{q}\hat{\epsilon}_{i}\tilde{z}_{i,q}\]</div>
<p>The test statistic is <span class="math notranslate nohighlight">\(nR^{2}\sim\chi_{q}^{2}\)</span> from the
regression.</p>
<p><strong>Anderson-Rubin</strong></p>
<p>The Andersen-Rubin test of overidentification examines the magnitude of
the LIML <span class="math notranslate nohighlight">\(\hat{\kappa}\)</span>which should be close to unity when the
model is not overidentified.</p>
<div class="math notranslate nohighlight">
\[n\ln(\hat{\kappa})\sim\chi_{q}^{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(q=p-k_{2}\)</span>.</p>
<p><strong>Basman’s F</strong></p>
<p>Basmann’s F test of overidentification also examines the magnitude of
the LIML <span class="math notranslate nohighlight">\(\hat{\kappa}\)</span>. The test statistic is</p>
<div class="math notranslate nohighlight">
\[\hat{\kappa}(n-p)/q\sim F_{q,n-n_{instr}}\]</div>
<p>where <span class="math notranslate nohighlight">\(q=p-k_{2}\)</span>.</p>
<p><strong>Durbin and Wu-Haussman</strong></p>
<p>Durbin’s and the Wu-Hausmann tests of exogeneity test of exogeneity is
depends on the variance of the residuals when come endogenous variables
are treated as exogenous against the case where they are treated as
endogenous. Durbin’s test statistic is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\delta= &amp; \hat{\epsilon}'_{e}P_{[z,w]}\hat{\epsilon}_{e}-\hat{\epsilon}'_{c}P_{z}\hat{\epsilon}_{c}\\
D= &amp; \delta/(\hat{\epsilon}'_{e}\hat{\epsilon}_{e})/n\sim\chi_{q}^{2}\end{aligned}\end{split}\]</div>
<p>and the Wu-Hausmann test statistic is</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
WH= &amp; \frac{\delta/q}{(\hat{\epsilon}'_{e}\hat{\epsilon}_{e}-\delta)/v}\sim F_{q,\nu}\end{aligned}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\epsilon}_{e}\)</span> treats the selected set of endogenous
variables as exogenous (efficient estimate) and
<span class="math notranslate nohighlight">\(\hat{\epsilon}_{c}\)</span> is a consistent estimator if these variables
are endogenous.<span class="math notranslate nohighlight">\(P_{\left[Z\,W\right]}\)</span> is the projection matrix
containing all exogenous variables including the instrument as well as
the variables being tested for endogeneity
<span class="math notranslate nohighlight">\(\left(W\right)\)</span>.<span class="math notranslate nohighlight">\(q\)</span> is the number of variables being
tested for exogeneity and <span class="math notranslate nohighlight">\(\nu=n-k1-k2-q\)</span>.</p>
</div>
<div class="section" id="gmm-post-estimation-results">
<h3>GMM Post-estimation Results<a class="headerlink" href="#gmm-post-estimation-results" title="Permalink to this headline">¶</a></h3>
<p><strong>J-stat</strong></p>
<p>The J-statistic tests whether the model is over-identified, and is
defined</p>
<div class="math notranslate nohighlight">
\[n\bar{g}'W^{-1}\bar{g}\sim\chi_{q}^{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{g}=n^{-1}\sum\hat{\epsilon}_{i}z_{i}\)</span> and <span class="math notranslate nohighlight">\(W\)</span> is
a consistent estimator of the variance of <span class="math notranslate nohighlight">\(\sqrt{n}\bar{g}\)</span>. The
degree of freedom is <span class="math notranslate nohighlight">\(q=p-k_{2}\)</span>.</p>
<p><strong>C-stat</strong></p>
<p>The C-statistic tests exogeneity by treating a the set of endogenous
variables as exogenous. In practice this means that are included in the
GMM moment conditions, and so a likelihood-ratio-like test statistic can
be computed as</p>
<div class="math notranslate nohighlight">
\[J_{e}-J_{c}\sim\chi_{m}^{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(J_{e}\)</span> is the J-statistic treating the tested variables as
exogenous and <span class="math notranslate nohighlight">\(J_{c}\)</span> leaves then as endogenous. The optimal
weighting matrix is computed from the expanded model (efficient) and
used to estimate parameters in both models. This ensures that the test
statistic is positive.</p>
</div>
</div>
<div class="section" id="first-stage-estimation-analysis">
<h2>First-stage Estimation Analysis<a class="headerlink" href="#first-stage-estimation-analysis" title="Permalink to this headline">¶</a></h2>
<p><strong>Partial R2 and Partial F-statistic</strong></p>
<p>The <span class="math notranslate nohighlight">\(R^{2}\)</span> is reported after orthogonalizing the instruments from
included exogenous variables, so that the model estimated is</p>
<div class="math notranslate nohighlight">
\[x_{2i}=\gamma_{0}+\tilde{z}_{2i}\gamma+\eta_{i}\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{Z}_{2}=M_{X_{1}}\tilde{Z}\)</span>. The partial
<span class="math notranslate nohighlight">\(F\)</span>-statistic is the F-statistic from this regression. It is
implemented as a standard <span class="math notranslate nohighlight">\(F\)</span>-statistic when the data is assumed
to be homoskedastic with an <span class="math notranslate nohighlight">\(F_{p_{2},n-p_{2}}\)</span> distribution. In
all other cases, a quadratic form is used with an asymptotic
<span class="math notranslate nohighlight">\(\chi_{p_{2}}^{2}\)</span> distribution testing <span class="math notranslate nohighlight">\(H_{0}:\gamma=0\)</span>.</p>
<p><strong>Shea’s R2</strong></p>
<p>Shea’s <span class="math notranslate nohighlight">\(R^{2}\)</span> is defined as the ratio of the parameter variances
under OLS and 2SLS estimation standardized by the unexplained variance
under each,</p>
<div class="math notranslate nohighlight">
\[\frac{\frac{\hat{\sigma}_{OLS,\beta_{i}}^{2}}{1-R_{OLS}^{2}}}{\frac{\hat{\sigma}_{IV,\beta_{i}}^{2}}{1-R_{IV}^{2}}}=\frac{\hat{\sigma}_{OLS,\beta_{i}}^{2}}{\hat{\sigma}_{IV,\beta_{i}}^{2}}\times\frac{1-R_{IV}^{2}}{1-R_{OLS}^{2}}\]</div>
<p>If the estimator under 2SLS was as good as under OLS, both ratios would
be 1 and Shea’s <span class="math notranslate nohighlight">\(R^{2}=1\)</span>. On the other hand, the worse the
<span class="math notranslate nohighlight">\(IV\)</span> fit in terms of either <span class="math notranslate nohighlight">\(R^{2}\)</span> or the parameter
variances, the lower the value reported by Shea’s <span class="math notranslate nohighlight">\(R^{2}\)</span>.</p>
</div>
<div class="section" id="kernel-weights-and-bandwidth-selection">
<h2>Kernel Weights and Bandwidth Selection<a class="headerlink" href="#kernel-weights-and-bandwidth-selection" title="Permalink to this headline">¶</a></h2>
<p><strong>Kernel weights</strong></p>
<p>In all formulas, <span class="math notranslate nohighlight">\(m\)</span> is the kernel bandwidth parameter.</p>
<ul>
<li><p class="first">Bartlett</p>
<div class="math notranslate nohighlight">
\[w_{i}=1-\frac{i}{m+1},\,i&lt;m\]</div>
</li>
<li><p class="first">Parzen</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
z_{i} &amp; =\frac{i}{m+1},\,i&lt;m\\
w_{i} &amp; =1-6z_{i}^{2}+6z_{i}^{3},z\leq0.5\\
w_{i} &amp; =2(1-z_{i})^{3},z&gt;0.5\end{aligned}\end{split}\]</div>
</li>
<li><p class="first">Quadratic-Spectral</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
z_{i} &amp; =\frac{6\pi i}{5m}\\
w_{0} &amp; =1\\
w_{i} &amp; =3(\sin(z_{i})/z_{i}-\cos(z_{i}))/z_{i}^{2},\:i\geq1\end{aligned}\end{split}\]</div>
</li>
</ul>
<p><strong>Optimal BW selection</strong></p>
<p>TODO</p>
</div>
<div class="section" id="constant-detection">
<h2>Constant Detection<a class="headerlink" href="#constant-detection" title="Permalink to this headline">¶</a></h2>
<p>Whether a model contains a constant or equivalent is tested using three
tests. These are executed in order and so once a constant is detected,
the others are not executed. The simplest method to ensure that a
constant is correctly detected is to include a columns of 1s.</p>
<ol class="arabic simple">
<li>A column with only 1.0s</li>
<li>A column with a maximum minus minimum equal to 0 and that is not all
0s.</li>
<li>Whether the rank of <span class="math notranslate nohighlight">\(X\)</span> is the same as
<span class="math notranslate nohighlight">\(\left[1_{N}\:X\right]\)</span>. If these are the same, then the model
contains a constant equivalent (e.g., dummies for all categories).</li>
</ol>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>If the model contains an implicit constant, e.g., all categories of a
dummy, one of the categories is excluded when computing the test
statistic. The choice of category to drop has no effect and is
equivalent to reparameterizing the model with a constant and
excluding one category of dummy.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[2]</a></td><td>This somewhat non-obvious choice is driven by Stata compatibility.</td></tr>
</tbody>
</table>
<p class="rubric">References</p>
<p>Sources used in writing the code include <a class="reference internal" href="../references.html#baltagi" id="id5">[Baltagi]</a>, <a class="reference internal" href="../references.html#baumetal" id="id6">[BaumEtAl]</a> and <a class="reference internal" href="../references.html#cametal" id="id7">[CamEtAl]</a>,
<a class="reference internal" href="../references.html#camtri05" id="id8">[CamTri05]</a>, <a class="reference internal" href="../references.html#camtri09" id="id9">[CamTri09]</a>, <a class="reference internal" href="../references.html#greene" id="id10">[Greene]</a>, <a class="reference internal" href="../references.html#newwes94" id="id11">[NewWes94]</a>, <a class="reference internal" href="../references.html#stata" id="id12">[Stata]</a>, <a class="reference internal" href="../references.html#wool10" id="id13">[Wool10]</a> and <a class="reference internal" href="../references.html#wool12" id="id14">[Wool12]</a>.</p>
</div>
</div>


          </div>
            
  <div class="footer-relations">
    
      <div class="pull-left">
        <a class="btn btn-default" href="gmm.html" title="previous chapter (use the left arrow)">GMM Weight and Covariance Estimation</a>
      </div>
    
      <div class="pull-right">
        <a class="btn btn-default" href="../panel/index.html" title="next chapter (use the right arrow)">Panel Data Model Estimation</a>
      </div>
    </div>
    <div class="clearer"></div>
  
        </div>
        <div class="clearfix"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../panel/index.html" title="Panel Data Model Estimation"
             >next</a> |</li>
        <li class="right" >
          <a href="gmm.html" title="GMM Weight and Covariance Estimation"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">linearmodels 4.8+11.g0239039 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" >Instrumental Variable Estimation</a> &#187;</li> 
      </ul>
    </div>
<script type="text/javascript">
  $("#mobile-toggle a").click(function () {
    $("#left-column").toggle();
  });
</script>
<script type="text/javascript" src="../_static/js/bootstrap.js"></script>
  <div class="footer">
    &copy; Copyright 2017, Kevin Sheppard. Created using <a href="http://sphinx.pocoo.org/">Sphinx</a>.
  </div>
  </body>
</html>